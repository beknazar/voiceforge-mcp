/**
 * Project scaffolding templates for different voice AI frameworks.
 */

export interface ProjectFile {
  path: string;
  content: string;
}

const LIVEKIT_STT_IMPORTS: Record<string, string> = {
  Deepgram: "from livekit.plugins import deepgram",
  OpenAI: "from livekit.plugins import openai",
  Google: "from livekit.plugins import google",
};

const LIVEKIT_LLM_IMPORTS: Record<string, string> = {
  OpenAI: "from livekit.plugins import openai",
  Anthropic: "from livekit.plugins import anthropic",
  Google: "from livekit.plugins import google",
};

const LIVEKIT_TTS_IMPORTS: Record<string, string> = {
  Cartesia: "from livekit.plugins import cartesia",
  ElevenLabs: "from livekit.plugins import elevenlabs",
};

const LIVEKIT_STT_SNIPPETS: Record<string, string> = {
  Deepgram: "deepgram.STT(model=\"{model}\")",
  OpenAI: "openai.STT(model=\"{model}\")",
  Google: "google.STT(model=\"{model}\")",
};

const LIVEKIT_LLM_SNIPPETS: Record<string, string> = {
  OpenAI: "openai.LLM(model=\"{model}\")",
  Anthropic: "anthropic.LLM(model=\"{model}\")",
  Google: "google.LLM(model=\"{model}\")",
};

const LIVEKIT_TTS_SNIPPETS: Record<string, string> = {
  Cartesia: "cartesia.TTS(model=\"{model}\")",
  ElevenLabs: "elevenlabs.TTS(model=\"{model}\")",
};

const LIVEKIT_ENV_KEYS: Record<string, string> = {
  Deepgram: "DEEPGRAM_API_KEY",
  AssemblyAI: "ASSEMBLYAI_API_KEY",
  OpenAI: "OPENAI_API_KEY",
  Google: "GOOGLE_API_KEY",
  Speechmatics: "SPEECHMATICS_API_KEY",
  Groq: "GROQ_API_KEY",
  Anthropic: "ANTHROPIC_API_KEY",
  Cartesia: "CARTESIA_API_KEY",
  ElevenLabs: "ELEVENLABS_API_KEY",
  PlayHT: "PLAYHT_API_KEY",
  Rime: "RIME_API_KEY",
};

const uniq = (items: string[]) => [...new Set(items.filter(Boolean))];

function templateOrError(stage: "STT" | "LLM" | "TTS", provider: string, model: string): string {
  if (stage === "STT") {
    const expression = LIVEKIT_STT_SNIPPETS[provider];
    if (!expression) throw new Error(`LiveKit scaffold does not support STT provider: ${provider}`);
    return expression.replace("{model}", model);
  }
  if (stage === "LLM") {
    const expression = LIVEKIT_LLM_SNIPPETS[provider];
    if (!expression) throw new Error(`LiveKit scaffold does not support LLM provider: ${provider}`);
    return expression.replace("{model}", model);
  }
  const expression = LIVEKIT_TTS_SNIPPETS[provider];
  if (!expression) throw new Error(`LiveKit scaffold does not support TTS provider: ${provider}`);
  return expression.replace("{model}", model);
}

export function getLiveKitAgentTemplate(config: {
  language: string;
  useCase: string;
  stt: string;
  sttModel: string;
  llm: string;
  llmModel: string;
  tts: string;
  ttsModel: string;
  agentName: string;
}): ProjectFile[] {
  const envKeyForProvider = (provider: string): string | null => {
    const key = LIVEKIT_ENV_KEYS[provider];
    return key ? `${key}=your-${provider.toLowerCase().replace(/\s/g, "-")}-key` : null;
  };

  const seen = new Set<string>();
  const addKey = (provider: string): string[] => {
    const line = envKeyForProvider(provider);
    if (!line || seen.has(provider)) return [];
    seen.add(provider);
    return [line];
  };

  const envLines = [
    "# VoiceForge Generated Config",
    `# Agent: ${config.agentName}`,
    `# Language: ${config.language} | Use Case: ${config.useCase}`,
    "",
    "# LiveKit",
    "LIVEKIT_URL=wss://your-project.livekit.cloud",
    "LIVEKIT_API_KEY=your-api-key",
    "LIVEKIT_API_SECRET=your-api-secret",
    "",
    `# STT: ${config.stt} ${config.sttModel}`,
    ...addKey(config.stt),
    "",
    `# LLM: ${config.llm} ${config.llmModel}`,
    ...addKey(config.llm),
    "",
    `# TTS: ${config.tts} ${config.ttsModel}`,
    ...addKey(config.tts),
    "",
  ];

  const sttImports = uniq([LIVEKIT_STT_IMPORTS[config.stt]]);
  const llmImports = uniq([LIVEKIT_LLM_IMPORTS[config.llm]]);
  const ttsImports = uniq([LIVEKIT_TTS_IMPORTS[config.tts]]);

  const envFile = envLines.join("\n");

  const agentPy = `"""
${config.agentName} - Voice AI Agent
Generated by VoiceForge MCP

Stack: ${config.stt} ${config.sttModel} → ${config.llm} ${config.llmModel} → ${config.tts} ${config.ttsModel}
Language: ${config.language} | Use Case: ${config.useCase}
"""

import logging
from dotenv import load_dotenv
from livekit.agents import AutoSubscribe, JobContext, WorkerOptions, cli, llm
from livekit.agents.pipeline import VoicePipelineAgent
${sttImports.filter(Boolean).join("\n")}
${llmImports.filter(Boolean).join("\n")}
${ttsImports.filter(Boolean).join("\n")}

load_dotenv()
logger = logging.getLogger("${config.agentName}")


async def entrypoint(ctx: JobContext):
    await ctx.connect(auto_subscribe=AutoSubscribe.AUDIO_ONLY)

    initial_ctx = llm.ChatContext().append(
        role="system",
        text=(
            "You are a voice AI agent for ${config.useCase.toLowerCase()}. "
            "You communicate in ${config.language}. "
            "Keep responses concise (1-2 sentences). "
            "Be natural, warm, and helpful."
        ),
    )

    agent = VoicePipelineAgent(
        vad=ctx.proc.userdata.get("vad"),
        stt=${templateOrError("STT", config.stt, config.sttModel)},
        llm=${templateOrError("LLM", config.llm, config.llmModel)},
        tts=${templateOrError("TTS", config.tts, config.ttsModel)},
        chat_ctx=initial_ctx,
    )

    agent.start(ctx.room)
    await agent.say(
        "Hello! How can I help you today?",
        allow_interruptions=True,
    )


if __name__ == "__main__":
    cli.run_app(WorkerOptions(entrypoint_fnc=entrypoint))
`;

  const LIVEKIT_PLUGIN_PACKAGES: Record<string, string> = {
    Deepgram: "livekit-plugins-deepgram>=1.0.0",
    OpenAI: "livekit-plugins-openai>=1.0.0",
    Cartesia: "livekit-plugins-cartesia>=1.0.0",
    ElevenLabs: "livekit-plugins-elevenlabs>=1.0.0",
    Anthropic: "livekit-plugins-anthropic>=1.0.0",
    Google: "livekit-plugins-google>=1.0.0",
  };

  const pluginLines = uniq([
    LIVEKIT_PLUGIN_PACKAGES[config.stt],
    LIVEKIT_PLUGIN_PACKAGES[config.llm],
    LIVEKIT_PLUGIN_PACKAGES[config.tts],
  ].filter(Boolean));

  const requirementsTxt = [
    "livekit-agents>=1.0.0",
    ...pluginLines,
    "livekit-plugins-silero>=1.0.0",
    "python-dotenv>=1.0.0",
    "",
  ].join("\n");

  const voiceforgeYaml = `# VoiceForge Agent Configuration
# Generated by VoiceForge MCP — https://getvoiceforge.com

agent:
  name: ${config.agentName}
  version: "1.0.0"
  language: ${config.language}
  use_case: ${config.useCase}

pipeline:
  stt:
    provider: ${config.stt.toLowerCase()}
    model: ${config.sttModel}
  llm:
    provider: ${config.llm.toLowerCase()}
    model: ${config.llmModel}
    temperature: 0.7
    max_tokens: 150
  tts:
    provider: ${config.tts.toLowerCase()}
    model: ${config.ttsModel}

quality:
  target_latency_ms: 250
  min_utmos: 4.0
  max_cost_per_min: 0.015

monitoring:
  log_level: info
  metrics: true
  alerts:
    latency_p95_threshold_ms: 350
    error_rate_threshold: 0.02
`;

  const readmeMd = `# ${config.agentName}

Voice AI agent generated by [VoiceForge](https://getvoiceforge.com).

## Stack

| Component | Provider | Model |
|-----------|----------|-------|
| STT | ${config.stt} | ${config.sttModel} |
| LLM | ${config.llm} | ${config.llmModel} |
| TTS | ${config.tts} | ${config.ttsModel} |

**Language:** ${config.language}
**Use Case:** ${config.useCase}

## Setup

\`\`\`bash
# Install dependencies
pip install -r requirements.txt

# Configure environment
cp .env.example .env
# Edit .env with your API keys

# Run the agent
python agent.py dev
\`\`\`

## Configuration

Edit \`voiceforge.yaml\` to adjust pipeline settings, quality targets, and monitoring thresholds.

## Powered by VoiceForge

This agent was scaffolded using VoiceForge MCP. To optimize your stack:

\`\`\`
npx voiceforge-mcp
\`\`\`
`;

  return [
    { path: ".env.example", content: envFile },
    { path: "agent.py", content: agentPy },
    { path: "requirements.txt", content: requirementsTxt },
    { path: "voiceforge.yaml", content: voiceforgeYaml },
    { path: "README.md", content: readmeMd },
  ];
}

export function getNextJSTemplate(config: {
  language: string;
  useCase: string;
  stt: string;
  sttModel: string;
  llm: string;
  llmModel: string;
  tts: string;
  ttsModel: string;
  agentName: string;
}): ProjectFile[] {
  const envFile = [
    "# VoiceForge Generated Config",
    `# Agent: ${config.agentName}`,
    "",
    "NEXT_PUBLIC_ELEVENLABS_AGENT_ID=your-agent-id",
    "ELEVENLABS_API_KEY=your-elevenlabs-key",
    ...(config.llm === "OpenAI" ? ["OPENAI_API_KEY=your-openai-key"] : []),
    ...(config.llm === "Anthropic" ? ["ANTHROPIC_API_KEY=your-anthropic-key"] : []),
    "",
  ].join("\n");

  const voiceWidgetTsx = `"use client";

import { useConversation } from "@elevenlabs/react";
import { useCallback, useState } from "react";

/**
 * ${config.agentName} — Voice Widget
 * Generated by VoiceForge MCP
 *
 * Stack: ${config.stt} → ${config.llm} → ${config.tts}
 */
export function VoiceWidget() {
  const [isActive, setIsActive] = useState(false);

  const conversation = useConversation({
    onConnect: () => setIsActive(true),
    onDisconnect: () => setIsActive(false),
    onError: (error) => console.error("Voice error:", error),
  });

  const startConversation = useCallback(async () => {
    try {
      await navigator.mediaDevices.getUserMedia({ audio: true });
      await conversation.startSession({
        agentId: process.env.NEXT_PUBLIC_ELEVENLABS_AGENT_ID!,
      });
    } catch (error) {
      console.error("Failed to start:", error);
    }
  }, [conversation]);

  const stopConversation = useCallback(async () => {
    await conversation.endSession();
  }, [conversation]);

  return (
    <div className="flex flex-col items-center gap-4 p-8">
      <div
        className={\`w-24 h-24 rounded-full flex items-center justify-center transition-all \${
          isActive
            ? "bg-red-500 animate-pulse shadow-lg shadow-red-500/50"
            : "bg-indigo-600 hover:bg-indigo-700 shadow-lg"
        }\`}
      >
        <button
          onClick={isActive ? stopConversation : startConversation}
          className="text-white text-sm font-medium"
        >
          {isActive ? "End" : "Start"}
        </button>
      </div>
      <p className="text-sm text-gray-500">
        {conversation.status === "connected"
          ? conversation.isSpeaking
            ? "Agent is speaking..."
            : "Listening..."
          : "Click to start"}
      </p>
    </div>
  );
}
`;

  return [
    { path: ".env.example", content: envFile },
    { path: "components/voice-widget.tsx", content: voiceWidgetTsx },
  ];
}
